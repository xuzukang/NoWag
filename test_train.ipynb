{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import yaml\n",
    "from src.utils import utils\n",
    "from src.model import llama\n",
    "from transformers import LlamaForCausalLM as OrigLlama\n",
    "import os\n",
    "from src import data\n",
    "import tqdm \n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=0,1\n",
    "# !export CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# torch.distributed.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "ft_n_train = 32\n",
    "ft_n_val = 32\n",
    "ft_dataset = \"pajama\"\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "seqlen = 4096\n",
    "batch_size = 1\n",
    "per_device_train_batch_size = 2\n",
    "use_embedding = False\n",
    "cache_logits_path = f\"temp/cache_logits_{ft_n_train}_{ft_n_val}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336ceefac41c4727aa891c39239c8ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_model = OrigLlama.from_pretrained(base_model,\n",
    "                                       device_map=\"auto\", torch_dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Red Pajama: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:04<00:00, 14.35it/s]\n"
     ]
    }
   ],
   "source": [
    "overall_data:list[torch.FloatTensor] = data.get_loaders(ft_dataset, nsamples = ft_n_train+ft_n_val\n",
    "                                  , model = base_model, train_test = \"train\",\n",
    "                                  seqlen=seqlen)\n",
    "\n",
    "overall_data = torch.stack([_[0][0] for _ in overall_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(ft_n_train+ft_n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating logits:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating logits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [03:40<00:00, 27.56s/it]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_logits(model: llama.LlamaForCausalLM, devset, batch_size):\n",
    "    logits = []\n",
    "    for i in tqdm.tqdm(range(len(devset) // batch_size), desc = \"Calculating logits\"):\n",
    "        logits.append(\n",
    "            model(devset[i * batch_size:(i + 1) *\n",
    "                         batch_size].cuda())['logits'].cpu())\n",
    "    logits = torch.concat(logits, dim=0)\n",
    "    return logits\n",
    "\n",
    "overall_out = calculate_logits(orig_model,overall_data, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(cache_logits_path):\n",
    "#     overall_out = torch.load(cache_logits_path)\n",
    "# else:\n",
    "#     overall_out = calculate_logits(orig_model,overall_data, batch_size)\n",
    "#     torch.save(overall_out, cache_logits_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4096, 32000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_out = overall_out[:, :-1].contiguous().softmax(dim=-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del orig_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import utils\n",
    "\n",
    "\n",
    "\n",
    "utils.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c15d7ecadf43ab9183c6684bf9ff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = llama.LlamaForCausalLM.from_pretrained(\"/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed_hf/run_38\",\n",
    "                                               device_map=\"auto\",\n",
    "                                                  torch_dtype=torch.float32,\n",
    "                                                    low_cpu_mem_usage=True)\n",
    "\n",
    "# model = model_utils.get_llama(\"meta-llama/Llama-2-7b-hf\",\n",
    "#                                device_map=\"auto\",\n",
    "#                                 dtype=torch.float32)\n",
    "                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    non_trained_outs = model(overall_data[[5]].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4095, 32000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out[[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 32000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_trained_outs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0019, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom kld loss\n",
    "def custom_kld_loss(outputs, labels, num_items_in_batch):\n",
    "    # print(outputs['logits'].shape)\n",
    "    logits = outputs['logits'][:,:-1,:].contiguous()\n",
    "    # print(logits.shape, labels.shape, num_items_in_batch)\n",
    "    \n",
    "    #take the cross entropy loss along the last dimension\n",
    "    #labels are of the same shape as logits\n",
    "    loss = -torch.sum(labels * torch.log_softmax(logits, dim=-1), dim=-1)\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "custom_kld_loss(non_trained_outs, overall_out[[5]].cuda(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7bd7ee998910>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(non_trained_outs.logits[0,0].softmax(dim=-1).cpu().numpy())\n",
    "plt.plot(overall_out[[5]][0,0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.4956e-09, 2.1544e-07, 1.7170e-04,  ..., 5.3435e-08,\n",
       "          2.1543e-08, 3.1212e-08],\n",
       "         [1.4783e-10, 6.7904e-10, 2.7640e-04,  ..., 2.5269e-08,\n",
       "          2.6011e-09, 1.1490e-09],\n",
       "         [5.7693e-11, 1.8938e-11, 3.6656e-05,  ..., 3.5489e-08,\n",
       "          3.3033e-09, 4.3084e-09],\n",
       "         ...,\n",
       "         [3.5009e-11, 1.2523e-11, 6.4294e-07,  ..., 2.0790e-10,\n",
       "          7.9399e-10, 3.7515e-11],\n",
       "         [5.5030e-11, 4.9690e-12, 4.8365e-07,  ..., 3.0510e-10,\n",
       "          1.3140e-09, 2.7364e-09],\n",
       "         [3.4451e-11, 4.7730e-12, 1.5994e-06,  ..., 3.4431e-10,\n",
       "          1.4308e-10, 1.8620e-09]],\n",
       "\n",
       "        [[2.5547e-10, 5.6940e-07, 6.2397e-04,  ..., 1.1917e-06,\n",
       "          4.8243e-08, 4.6965e-07],\n",
       "         [4.7125e-09, 2.0675e-08, 7.5365e-05,  ..., 3.3496e-07,\n",
       "          6.5462e-08, 2.4501e-06],\n",
       "         [7.7880e-09, 1.3783e-08, 3.8610e-05,  ..., 1.2072e-06,\n",
       "          7.2169e-07, 8.2109e-06],\n",
       "         ...,\n",
       "         [2.7569e-11, 8.4196e-12, 4.6826e-07,  ..., 1.5128e-09,\n",
       "          5.3274e-09, 1.4576e-09],\n",
       "         [3.2274e-14, 3.7425e-14, 1.5471e-07,  ..., 1.5510e-12,\n",
       "          2.8584e-12, 2.4696e-12],\n",
       "         [3.4002e-12, 3.2680e-12, 1.0049e-07,  ..., 7.0798e-11,\n",
       "          1.2594e-11, 1.8451e-10]],\n",
       "\n",
       "        [[4.2689e-09, 1.5715e-06, 1.0836e-03,  ..., 9.8285e-08,\n",
       "          2.0883e-07, 2.7171e-07],\n",
       "         [4.1286e-11, 8.4261e-10, 2.8693e-05,  ..., 9.8025e-10,\n",
       "          1.4892e-09, 1.0412e-10],\n",
       "         [2.7889e-11, 1.9017e-10, 3.9240e-05,  ..., 1.2233e-08,\n",
       "          8.4012e-10, 5.7356e-10],\n",
       "         ...,\n",
       "         [1.7417e-09, 2.7135e-09, 5.1145e-07,  ..., 1.7947e-08,\n",
       "          6.5739e-09, 1.0160e-08],\n",
       "         [1.0727e-10, 2.9247e-11, 2.4143e-07,  ..., 7.9857e-09,\n",
       "          3.1820e-09, 1.6199e-08],\n",
       "         [2.0225e-10, 1.5480e-10, 2.4183e-07,  ..., 1.7132e-08,\n",
       "          2.6956e-09, 1.0506e-08]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.9853e-09, 3.7926e-08, 1.4935e-05,  ..., 1.7469e-07,\n",
       "          1.1067e-08, 4.8097e-08],\n",
       "         [4.8645e-10, 4.3309e-10, 1.1240e-05,  ..., 1.2789e-08,\n",
       "          9.9331e-09, 3.6744e-08],\n",
       "         [6.4425e-10, 9.1979e-10, 7.3853e-13,  ..., 1.5033e-09,\n",
       "          1.1326e-09, 1.1293e-09],\n",
       "         ...,\n",
       "         [3.2666e-14, 3.5625e-13, 2.8248e-09,  ..., 4.6255e-13,\n",
       "          5.9930e-13, 1.6714e-12],\n",
       "         [5.2380e-11, 9.4309e-12, 6.9645e-08,  ..., 2.9359e-11,\n",
       "          2.9083e-09, 2.5213e-10],\n",
       "         [2.6100e-11, 2.5223e-12, 1.9622e-07,  ..., 1.9947e-11,\n",
       "          4.6621e-10, 1.5532e-10]],\n",
       "\n",
       "        [[2.9091e-10, 2.3422e-10, 2.8640e-06,  ..., 8.1564e-09,\n",
       "          1.3848e-08, 3.2186e-08],\n",
       "         [9.2730e-11, 4.2242e-11, 7.5982e-04,  ..., 1.4679e-09,\n",
       "          1.4515e-09, 6.5100e-09],\n",
       "         [3.5278e-11, 6.0216e-10, 1.5822e-02,  ..., 2.5569e-08,\n",
       "          1.6056e-08, 2.0379e-08],\n",
       "         ...,\n",
       "         [1.4542e-13, 2.3095e-12, 1.8162e-07,  ..., 2.5551e-13,\n",
       "          4.0609e-14, 1.1108e-12],\n",
       "         [1.7776e-12, 6.0908e-13, 7.3315e-06,  ..., 5.2668e-12,\n",
       "          1.5722e-12, 1.9220e-11],\n",
       "         [1.7923e-11, 1.0474e-11, 4.7229e-06,  ..., 6.3481e-11,\n",
       "          6.2398e-11, 1.4583e-10]],\n",
       "\n",
       "        [[6.9762e-10, 1.2850e-09, 3.3276e-04,  ..., 3.1891e-08,\n",
       "          2.6717e-08, 3.7975e-09],\n",
       "         [1.4889e-10, 3.2341e-10, 1.0543e-05,  ..., 8.7346e-09,\n",
       "          2.4354e-09, 2.3691e-09],\n",
       "         [1.3916e-11, 7.0837e-11, 1.2947e-05,  ..., 1.3058e-10,\n",
       "          1.6427e-10, 6.6946e-11],\n",
       "         ...,\n",
       "         [7.8291e-11, 3.5988e-12, 8.7884e-08,  ..., 4.4402e-11,\n",
       "          7.7008e-11, 5.8144e-12],\n",
       "         [3.4180e-12, 5.8115e-13, 2.0151e-07,  ..., 3.4306e-11,\n",
       "          1.3765e-11, 5.0508e-12],\n",
       "         [8.4727e-12, 1.6173e-12, 2.6744e-07,  ..., 2.1599e-11,\n",
       "          2.5066e-11, 8.9023e-12]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, soft_labels):\n",
    "        self.inputs = inputs\n",
    "        self.soft_labels = soft_labels\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx],\n",
    "            'labels': self.soft_labels[idx],\n",
    "        }\n",
    "    \n",
    "def make_datasets(X:torch.FloatTensor, Y:torch.FloatTensor, n_val:int) -> Tuple[Dataset, Dataset]:\n",
    "    \n",
    "\n",
    "    #make the indices\n",
    "    idxs = torch.randperm(len(X))\n",
    "    train_idxs = idxs[:-n_val]\n",
    "\n",
    "    train_ds = SimpleDataset(X[train_idxs], Y[train_idxs])\n",
    "    valid_ds = SimpleDataset(X[idxs[-n_val:]], Y[idxs[-n_val:]])\n",
    "    return train_ds, valid_ds\n",
    "\n",
    "traindataset, validdataset = make_datasets(overall_data, overall_out, ft_n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(ft_n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(overall_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset), len(validdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm6481\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/lliu/huffman/wandb/run-20250323_175729-jgo95o13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m6481/huggingface/runs/jgo95o13' target=\"_blank\">llama-2-7b-hf-soft</a></strong> to <a href='https://wandb.ai/m6481/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m6481/huggingface' target=\"_blank\">https://wandb.ai/m6481/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m6481/huggingface/runs/jgo95o13' target=\"_blank\">https://wandb.ai/m6481/huggingface/runs/jgo95o13</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/40 02:55 < 12:55, 0.04 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.736315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.844500</td>\n",
       "      <td>1.732244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.786100</td>\n",
       "      <td>1.727216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#train the model on the dataset with transformers trainer\u001b[39;00m\n\u001b[32m      3\u001b[39m trainer = transformers.Trainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     args=transformers.TrainingArguments(\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     compute_loss_func=custom_kld_loss,\n\u001b[32m     36\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:2647\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2644\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2646\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2650\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2651\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3100\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3097\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3101\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3197\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3195\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3196\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3197\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3200\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3884\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3881\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3883\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3988\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3986\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   3987\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3988\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3993\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/modeling_utils.py:3578\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3573\u001b[39m     gc.collect()\n\u001b[32m   3575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3576\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3577\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3578\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3580\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/safetensors/torch.py:500\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    499\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    501\u001b[39m     }\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/safetensors/torch.py:422\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#train the model on the dataset with transformers trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"./output\",\n",
    "        num_train_epochs=10,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # eval_steps=100,\n",
    "        # per_device_train_batch_size=32,\n",
    "        load_best_model_at_end=True,\n",
    "        #add a tqdm progress bar\n",
    "        #set the lr to 1e-5\n",
    "        learning_rate=1e-6,\n",
    "        #constat lr\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        # warmup_steps=100,\n",
    "        dataloader_pin_memory=True,\n",
    "        #set the logging dir to ./logs\n",
    "        logging_dir=\"./logs\",\n",
    "        #log to wandb\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"llama-2-7b-hf-soft\",\n",
    "        eval_on_start = True,\n",
    "    ),\n",
    "    train_dataset=traindataset,\n",
    "    eval_dataset=validdataset,\n",
    "    compute_loss_func=custom_kld_loss,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NoWAC-VQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
